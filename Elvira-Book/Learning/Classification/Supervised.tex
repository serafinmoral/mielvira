\section{Supervised Classification}{\em Elvira} encloses five classic and well-known supervisedclassification algorithms which can be acceded by the graphicalinterface and the on-line command tool. The algorithms are thenaive Bayes \cite{cestnik90}, the selective naive Bayes\cite{langley94}, tree-augmented naive Bayes (TAN)\cite{friedman97}, the semi naive Bayes algorithm \cite{pazzani95}and the {\em k}-dependence Bayesian classifier (KDB)\cite{sahami96}. Coupled with the induction processes of theseclassifiers, {\em Elvira} encloses several facilities which areessential in a complete supervised classification study such as:\begin{itemize}\item estimation of the predictive accuracy percentage of theclassifier; \item display of the confusion matrix derived from theprevious estimation process; \item after the classifier islearned, categorization of a datafile of unlabelled set of caseswith the class value predicted by the model for each case; \itemafter the classifier is learned, given an unseen dataset oflabeled test cases, check the (in)equality of the given label andthe class value predicted by the model.\end{itemize}Two possibilities exist in the graphical interface of {\em Elvira}to easiliy access these facilities: \begin{enumerate} \item Thethird button of the icons-row of the graphical interface, that is,the {\em Options for the database cases file} button, allows theusage of these facilities. \item The facilities are alsoaccessible by clicking in {\em File} $\rightarrow$ {\em Open casesfile}. \end{enumerate}These two possibilities open an user-friendly window which showsthe {\em machine learning} and {\em post learning} options tohandle the desired data and the induced models.{\em Elvira} has a set of several Java classes which implementthese classifiers and facilities.The {\em ClassifierValidator} and {\em  ConfusionMatrix} classesdeal with the accuracy estimation process and the calculation ofthe associated confusion matrix, respectively. Both classes arelocated in the {\em elvira.learning.classification} package.By means of the {\em ClassifierValidator} class, {\em Elvira}allows the hold-out, {\em k}-fold cross-validation andleave-one-out validation schemes. The next lines are used todescribe the principal methods associated to the {\emClassifierValidator} class:\begin{itemize}\item public ClassifierValidator(Classifier classifier, Vectordbcs, int classvar, int method); This class builder fixes theclassifier to be validated, a vector of {\em DataBaseCases}-typeobjects, the position of the class label in the order of thevariables in the dataset, and the specific validation method to beused. When a hold-out validation scheme is used, the {\em dbcs}parameter hosts two {\em databasecases} objects, which representthe training and testing subsets of cases, respectively; when {\emk}-fold cross-validation or leave-one-out validation proceduresare used, the {\em dbcs} parameter represents a single set ofcases. \item public void setSeed(long seed); In order to deal withthe needed validation procedure, this method sets a new seed toperform the needed random instance partitions in the dataset.\item public Vector trainAndTest(); This method implements ahold-out validation procedure, returning in the {\em Vector}object the confusion matrices in the training and test subsets.\item public ConfusionMatrix kFoldSumCrossValidation (int k); Thismethod implements a {\em k}-fold cross-validation procedure,returning the confusion matrix associated to the validationprocess. The returned confusion matrix hosts the sum of {\em k}confusion matrices generated in the {\em k} partitions of the {\emk}-fold cross-validation process. \item public VectorkFoldCrossValidation\_Vector (int k); This method returns in a{\em Vector} object the {\em k} confusion matrices generated inthe {\em k} partitions of the {\em k}-fold cross-validationprocess. \item public ConfusionMatrix leaveOneOutSum(); Thismethod implements a leave-one-out procedure, returning theconfusion matrix associated to the validation process, which hoststhe sum of the confusion matrices generated for each instance ofthe leave-one-out accuracy estimation process. \item public VectorleaveOneOut\_Vector(); This method returns in a {\em Vector}object the confusion matrices generated for each instance of theleave-one-out accuracy estimation process. \item public doubleerror (Classifier classifier, DataBaseCases dbc, int classnumber);This method computes the error rate for a classifier in a given{\em DataBaseCases} object to be categorized. The {\emclassnumber} object indicates the position of the class label inthe order of the variables in the dataset. \item publicConfusionMatrix confusionMatrix (Classifier classifier,DataBaseCases dbc, int classnumber); Using the same arguments asthe previous method, this {\em ConfusionMatrix} method returns theconfusion matrix associated to the exposed categorization processover a {\em DataBaseCases} object.\end{itemize}By means of the {\em ConfusionMatrix} class, {\em Elvira} allowsthe display of the confusion matrix which is the consequence ofthe used accuracy estimation scheme. The next lines are used todescribe the principal methods associated to the {\emConfusionMatrix} class:\begin{itemize}\item public ConfusionMatrix(FiniteStates classVariable); Thisclass builder has as parameter the class label itself. \itempublic ConfusionMatrix(int classNumber); This class builder has asparameter the number of different values of the class label. \itempublic double getError(); This method prints in the command linethe error rate stored in the confusion matrix; \item public doublegetStandardDeviation(); This method prints the standard deviationstored in the confusion matrix; \item public void print(); Bymeans of this method, the confusion matrix is printed in thecommand line.\end{itemize}The {\em Naive\_Bayes, SelectiveNaiveBayes,WrapperSelectiveNaiveBayes, TAN, CMutInfTAN, SemiNaiveBayes,WrapperSemiNaiveBayes, KDB} and {\em CMutInfKDB} classes deal withthe induction processes of the different classification modelsfrom a dataset of labeled cases. Four pairs of these classescollaborate in the construction and use of the mentionedclassification models. These classes are located in the {\emelvira.learning.classification.supervised.discrete} package. Inorder to induce and manage these supervised classifiers, andlocated in the same package as previous classes, the {\emDiscreteClassifier} abstract class hosts the generalcharacteristics and methods associated to a common supervisedclassification process. Thus, the exposed classes associated tothe induction of different supervised classifiers, are children ofthis abstract class. The rest of this section is used to presentthe principal methods associated to these classes, associated withthe exposed classifier structures.{\em DiscreteClassifier} is an abstract class, parent of the restclasses which finally implement each of the supervised discreteclassifiers. (Note that these classes not allow the use ofcontinuous predictive variables.) This class hosts the facilitiesand procedures which are general to any supervised classificationtask (several of them were described in the previous paragraphs).The following lines are used to describe the principal methods ofthis general-purpose supervised-classification class:\begin{itemize}\item public DiscreteClassifier(); It is an empty builder for theclass. \item public DiscreteClassifier(DataBaseCases data, booleanlap); This class builder fixes the dataset of cases to be used andthe possibility to use the Laplace correction in the learning ofthe probabilities. The builder also checks whether the variablesof the dataset have discrete values. \item public voidsetClassifier(Bnet model); This method is used to assign theBnet-type object as a classification model. \item public BnetgetClassifier(); This method returns a classification model as aBnet-type object. \item public abstract void structuralLearning();This abstract method, which is implemented in the subclasses,learns the graphical structure of a model. However, it does notlearn the associated (conditional) probability distributiontables. \item public void parametricLearning(); This method isimplemented in the subclasses of DiscreteClassifier class. Itfills, based on the dataset given to the class builder, the(conditional) probability distribution tables associated to thegraphical structure learned by the previous structuralLearning()method. The (conditional) distribution tables appear in the BNetobject. \item public void train(); In order to learn a classifier,this method calls, in the given order, both structuralLearning()and parametricLearning() methods. \item public voidlearn(DataBaseCases data, int classIndex); The result of thismethod is the same as first using the non-empty class builder,followed by the train() method. \item public int assignClass(double[] caseTest); Given the array object caseTest (whichrepresents an unlabeled instance), this methods returns, accordingto a classifier previously learned by the train() method, the mostlikely class label. \item public int assignClass(Configurationconf); This method extracts an array of double-type numbers fromthe given Configuration-type object, and it calls the previous\\assignClass(double[] caseTest) method. \item public doubletest(DataBaseCases test); Once a classifier has been learned, thismethod tests its accuracy over a labeled set of examples given inthe DataBaseCases-type object. A confusion matrix is alsoreturned. \item public void categorize(String inputFile, StringoutputFile); Once a classifier has been learned, this method canbe used to categorize (predict the most likely label value) adatafile of cases given in the {\em inputFile} object. Thepredicted class value is written as the last variable of anexample, that is, at the end of each line-sample. The categorizedfile is returned in the {\em outputFile} object. \item publicConfusionMatrix getConfusionMatrix(); This method, by means of the{\em test(data)} method, returns a confusion matrix. \itemabstract Vector classify(Configuration instance, int classnumber);It returns an ordered vector which collects, for a given {\eminstance}, the probability belonging to each class of the problem.Note that it is assumed that the first class of the problem islabeled as zero (class 0).\end{itemize}{\em Naive\_Bayes} class implements the well-known naive Bayes(NB) \cite{cestnik90} classifier. It uses a variation of the Bayesrule to predict the class for a test instance, assuming thatfeatures are conditionally independent to each other given theclass. NB applies the following rule: \[c_{NB} = \arg \max_{c^{j}\in C} p(c^{j}) \prod_{i=1}^{n} p(x_{i}|c^{j})\] where $c_{NB}$denotes the category value predicted by the naive Bayes classifierfor a test instance. The probability for discrete features isestimated from data using maximum likelihood estimation orapplying the Laplace correction when it is requested by the user.Unknown values in the test instance are skipped. Although itssimplicity and its conditionally independence assumption amongvariables, the literature shows that the NB classifier givesremarkably high accuracies in many domains \cite{langley94},specially in medical ones. The {\em Naive\_Bayes} class hosts thebasic facilities to build and manage a naive Bayes supervisedclassification model. The following lines are used to describe theprincipal methods of this class:\begin{itemize}\item public Naive\_Bayes(); It is an empty builder for the class.\item public Naive\_Bayes(DataBaseCases data, boolean lap); Thisclass builder fixes the dataset of cases to be used and thepossibility to use the Laplace correction in the learning of theneeded probabilities for the naive Bayes model. \item public voidstructuralLearning(); This method learns the naive Bayesstructure. A {\em Bnet} object is built, where each predictivevariable is linked as children node of the class variable. \itempublic void parametricLearning(); This method learns the(conditional) probability distribution tables associated to thenaive Bayes model. \item public static void main(String[] args);This class is useful when {\em Elvira} is used from the commandline. Given as input a training and test files, this method writesa file in {\em Elvira} {\em elv} format with the learned naiveBayes model. The associated confusion matrix is written in thecomputer screen.\end{itemize}{\em SelectiveNaiveBayes} class implements, with the collaborationof the class {\em WrapperSelectiveNaiveBayes}, the modification ofthe classic naive Bayes approach presented by Langley and Sage(1994), where not all the predictive variables are included in thefinal naive Bayes classification model, which maintains itsconditional independence assumption among the variables (given theclass). A forward hill-climbing search process is performed guidedby a {\em k}-fold cross-validation accuracy estimation, startingfrom the empty subset of variables, in the space of variablesubsets. The purpose of this algorithm is to discover the groupsof correlated features which damage the accuracy of a naive Bayesclassifier. The following lines are used to describe the principalmethods of the {\em SelectiveNaiveBayes} class:\begin{itemize}\item public SelectiveNaiveBayes(DataBaseCases data, boolean lap);This class builder fixes the dataset of cases to be used and thepossibility to use the Laplace correction in the learning of theneeded probabilities for the exposed selective naive Bayes model.\item public abstract void structuralLearning(); This abstractmethod is implemented in the {\em WrapperSelectiveNaiveBayes}subclass, learning the naive Bayes structure using only the subsetof selected predictive variables. \item public voidparametricLearning(); So related with the previous one, thismethod learns the (conditional) probability distribution tablesassociated to the selective naive Bayes model.\end{itemize}{\em WrapperSelectiveNaiveBayes} is a child-class of the {\emSelectiveNaiveBayes} class. Its principal methods are thefollowing:\begin{itemize}\item public WrapperSelectiveNaiveBayes(); It is an empty builderfor the class. \item publicWrapperSelectiveNaiveBayes(DataBaseCases data, boolean lap, intk); This class builder calls the class builder with the parameters{\em data} (dataset of cases), {\em lap} (possibility of using theLaplace correction in the parametric learning process) and {\em k}(the number of folds used in the cross-validation process whichguides the search procedure). \item void structuralLearning();This method learns the selective naive Bayes structure, building a{\em Bnet} object according to the algorithm proposed by Langleyand Sage (1994), where the search process is performed in theexposed wrapper form, guided by the estimated accuracy of thefound structure. \item public static void main(String[] args);This class is useful when {\em Elvira} is used from the commandline. Given as input a training and test files, this method writesa file in {\em Elvira} {\em elv} format with the learned selectivenaive Bayes structure. The associated confusion matrix is alsowritten in the computer screen.\end{itemize}{\em TAN} class implements, with the collaboration of the {\emCMutInfTAN} class, the classic Tree Augmented Network (TAN)\cite{friedman97} classifier. This algorithm goes beyond the naiveBayes classifier as it allow the existence of probabilisticrelationships between the predictive variables: the procedure forlearning these relationships is based on a method reported by Chowand Liu \cite{chow68}. The authors describe a procedure forconstructing an optimal dependency tree structure in the sensethat among all possible trees, it learns the probabilistic treestructure that best approximates the data for predictivevariables. The authors use the Kullback-Leibler cross-entropymeasure as a distance criterion between the probabilitydistribution of the database and the probability distributioninduced by the tree-like structure. Then, the class variable islinked as parent to every predictive variable. The following linesare used to describe the principal methods of the {\em TAN} class:\begin{itemize}\item public TAN(); It is an empty builder for the class. \itempublic TAN(DataBaseCases data, boolean lap); This class builderfixes the dataset of cases to be used and the possibility to usethe Laplace correction in the learning of the needed probabilitiesfor the tree augmented network model. \item protected booleanmakesCycle(Graph tree, Node head, Node tail); This method returnstrue when the inclusion of the arc {\em head} $\rightarrow$ {\emtail} creates a cycle in the tree structure. \item public abstractvoid structuralLearning(); This abstract method is implemented inthe {\em CMutInfTAN} subclass, learning the tree augmented networkstructure. \item public void parametricLearning(); So related withthe previous one, this method learns the (conditional) probabilitydistribution tables associated to the tree augmented networkmodel.\end{itemize}{\em CMutInfTAN} is a child-class of the {\em TAN} class. Itsprincipal methods are the following:\begin{itemize}\item public CMutInfTAN(); It is an empty builder for the class.\item public CMutInfTAN(DataBaseCases data, boolean lap); Thisclass builder calls the {\em TAN} builder with the parameters {\emdata} (dataset of cases) and {\em lap} (possibility of using theLaplace correction in the parametric learning process). \item voidstructuralLearning(); This method learns the tree augmentednetwork structure, building a {\em Bnet} object according to thealgorithm proposed by Friedman et al. (1997). \item public staticvoid main(String[] args); This class is useful when {\em Elvira}is used from the command line. Given as input a training and testfiles, this method writes a file in the {\em Elvira's} {\em elv}format with the learned tree augmented network model according tothe work of Friedman et al. (1997). The associated confusionmatrix is also written in the computer screen.\end{itemize}{\em SemiNaiveBayes} class implements, with the collaboration ofthe class {\em WrapperSemiNaiveBayes}, the classication approachpresented by Pazzani (1995), called Semi naive Bayes. Thisclassifier tries to discover in a wrapper way, the dependenciesamong predictive features, grouping highly correlated features ina joint variable (node), which encodes the cartesian product ofdependent (grouped) variables. The classifier also allows theexclusion of irrelevant predictive variables from the model. Agreedy search process, guided by a {\em k}-fold cross-validationaccuracy estimation, is conducted in the space of allowedstructures, considering the following two operators at each stepof the search:\begin{enumerate}\item Add a variable not used by the current classifier as a newvariable class conditionally independent of all other variablesused in the classifier. \item Join a variable not used by thecurrent classifier with a variable currently used by theclassifier. \end{enumerate}{\em Elvira's} implementation gathers the forward greedy searchapproach of Pazzani's (1995) proposal. The following lines areused to describe the principal methods of the {\em SemiNaiveBayes}class:\begin{itemize}\item public SemiNaiveBayes(DataBaseCases data, boolean lap); Thisclass builder fixes the dataset of cases to be used and thepossibility to use the Laplace correction in the learning of theneeded probabilities for the exposed semi naive Bayes model. \itempublic abstract void structuralLearning(); This abstract method isimplemented in the {\em WrapperSemiNaiveBayes} subclass, learningthe exposed semi naive Bayes structure. \item public voidparametricLearning(); So related with the previous one, thismethod learns the (conditional) probability distribution tablesassociated to the semi naive Bayes model. \item public intassignClass(double[] caseTest); In order to deal with thejoin-nodes formed by cartesian products of original variables,this method is redefined in the {\em SemiNaiveBayes} class topredict, according to a classifier previously learned, the mostlikely class label for a given unlabeled instance.\end{itemize}{\em WrapperSemiNaiveBayes} is a child-class of the {\emSemiNaiveBayes} class. Its principal methods are the following:\begin{itemize}\item public WrapperSemiNaiveBayes(); It is an empty builder forthe class. \item public WrapperSemiNaiveBayes(DataBaseCases data,boolean lap, int k); This class builder calls the class builderwith the parameters {\em data} (dataset of cases), {\em lap}(possibility of using the Laplace correction in the parametriclearning process) and {\em k} (the number of folds used in thecross-validation process which guides the search procedure). \itemvoid structuralLearning(); This method learns the semi naive Bayesstructure, building a {\em Bnet} object according to the algorithmproposed by Pazzani (1995), where the search process is performedin the exposed wrapper form, guided by the estimated accuracy ofthe found structure. \item public static void main(String[] args);This class is useful when {\em Elvira} is used from the commandline. Given as input a training and test files, this method writesa file in {\em Elvira} {\em elv} format with the learned seminaive Bayes structure. The associated confusion matrix is writtenin the computer screen.\end{itemize}{\em KDB} class implements, with the collaboration of the {\emCMutInfKDB} class, the {\em k}-dependence classifier (KDB)proposed by Sahami (1996). The KDB structure can be seen as anaive Bayes structure which allows each predictive variable tohave a maximum of {\em k} predictor variables as parents, apartfrom the class label, which is linked as parent to every variable:thus, it allows the construction of classifiers at arbitraryvalues for the maximum number of dependencies between variables.The algorithm proposed by Sahami (1996) uses the class conditionalmutual information mesaure between pairs of predictive variablesand the mutual information between the class label and a singlepredictive variable to lead the construction of the classifier.The following lines are used to describe the principal methods ofthe {\em KDB} class:\begin{itemize}\item public KDB(int k); It is an empty builder for the class,where {\em k} denotes the exposed maximum number of parents foreach predictive variable. \item public KDB(DataBaseCases data,boolean lap, int k); This class builder fixes the dataset of casesto be used, the possibility to use the Laplace correction in thelearning of the needed probabilities for the {\em k}-dependenceclassifier model, and the maximum number of parents for eachpredictive variable.  \item public abstract voidstructuralLearning(); This abstract method is implemented in the{\em CMutInfKDB} subclass, learning the {\em k}-dependenceclassifier structure. \item public void parametricLearning(); Sorelated with the previous one, this method learns the(conditional) probability distribution tables associated to the{\em k}-dependence classifier model.\end{itemize}{\em CMutInfKDB} is a child-class of the {\em KDB} class. Itsprincipal methods are the following:\begin{itemize}\item public CMutInfKDB(); It is an empty builder for the class.\item public CMutInfKDB(DataBaseCases data, boolean lap, int k);This class builder calls the {\em TAN} builder with the parameters{\em data} (dataset of cases), {\em lap} (possibility of using theLaplace correction in the parametric learning process) and {\em k}(maximum number of parents for each predictive variable). \itemvoid structuralLearning(); This method learns the {\emk}-dependence classifier structure, building a {\em Bnet} objectaccording to the algorithm proposed by Sahami (1996). \item publicstatic void main(String[] args); This class is useful when {\emElvira} is used from the command line. Given as input a trainingand test files, this method writes a file in {\em Elvira} {\emelv} format with the learned {\em k}-dependence classifier modelaccording to the work of Sahami (1996). The associated confusionmatrix is written in the computer screen.\end{itemize}%- Procedimientos 'teóricos' para el problema.%%    Algoritmo implementado en Elvira.%%    Psudocódigo del algoritmo.%%    Referencias bibliográficas.%- Estructura de clases Principales para resolver el problema.%-Uso del algoritmo.%%    Uso en línea de comandos como API%%    Uso desde el interface gráfico